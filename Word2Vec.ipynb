{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import codecs\n",
    "import glob\n",
    "import multiprocessing\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sankyfox/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sankyfox/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./sherlock_holmes.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_files = sorted(glob.glob(\"./*.txt\")) \n",
    "raw_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if txt files are utf-8\n",
    "# import chardet\n",
    "# for f in raw_data_files:\n",
    "#     chardet.detect(open(f).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading './sherlock_holmes.txt' ...\n",
      "Corpus is now 3868122 characters long\n"
     ]
    }
   ],
   "source": [
    "raw_corpus = u\"\"\n",
    "for file_name in raw_data_files:\n",
    "    print(\"Reading '{0}' ...\".format(file_name))\n",
    "    with codecs.open(file_name,\"r\",\"utf-8\") as f:\n",
    "        raw_corpus += f.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(raw_corpus)))\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "raw_sentences = tokenizer.tokenize(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \",raw)\n",
    "    words = clean.split()\n",
    "    words = [x.lower() for x in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus has 668,430 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(s) for s in sentences])\n",
    "print(\"The corpus has {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "num_features = 300\n",
    "min_word_count = 3\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "context_size = 20\n",
    "downsampling = 1e-3\n",
    "seed = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = w2v.Word2Vec(sg=1,seed=seed,workers=num_workers,size=num_features,min_count=min_word_count,window=context_size,sample=downsampling,iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length:  9603\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length: \",len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9288329"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the trained file so we can load it anytime\n",
    "if not os.path.exists('trained'):\n",
    "    os.mkdir('trained')\n",
    "model.save(os.path.join(\"trained\",\"model.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = w2v.Word2Vec.load(os.path.join(\"trained\",\"model.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for some fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sherlock:Irene:: Watson: ?\n",
      "holmes\n",
      "Well we always suspected that ...\n",
      "\n",
      "Let's find all doctors\n",
      "Inspector:Lestrade :: Dr: ?\n",
      "[(u'mortimer', 0.4274638295173645), (u'roylott', 0.4094651937484741), (u'huxtable', 0.3990991711616516), (u'sterndale', 0.3790780305862427), (u'leon', 0.3714893162250519), (u'starr', 0.3627554178237915), (u'reminiscences', 0.34268635511398315), (u'armstrong', 0.3406115174293518), (u'grimesby', 0.33381372690200806), (u'correspondent', 0.3333827257156372)]\n",
      "\n",
      "How about professors ?\n",
      "Inspector:Lestrade :: professor: ?\n",
      "[(u'coram', 0.4137895405292511), (u'moriarty', 0.37892886996269226), (u'presbury', 0.3314708471298218), (u'roy', 0.3195204436779022), (u'lamented', 0.30523109436035156), (u'propped', 0.30168890953063965), (u'drift', 0.2694909870624542), (u'sebastian', 0.25961148738861084), (u'cousin', 0.25885090231895447), (u'solutions', 0.25093913078308105)]\n",
      "\n",
      "Our model recongnizes inspectors and knows Watson isn't one\n",
      "Given : gregson watson lestrade baynes\n",
      "Output :  watson\n",
      "\n",
      "Wow! it knows the difference between the good guys and the bad guys !\n",
      "Given : moritarty holmes augustus sebastian\n",
      "Output :  holmes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Sherlock:Irene:: Watson: ?\")\n",
    "print(model.wv.most_similar(positive=['sherlock', 'watson'], negative=['irene'])[0][0])\n",
    "print(\"Well we always suspected that ...\")\n",
    "print()\n",
    "\n",
    "print(\"Let's find all doctors\")\n",
    "print(\"Inspector:Lestrade :: Dr: ?\")\n",
    "print(model.wv.most_similar(positive=['inspector','dr'], negative=['lestrade']))\n",
    "print()\n",
    "\n",
    "print(\"How about professors ?\")\n",
    "print(\"Inspector:Lestrade :: professor: ?\")\n",
    "print(model.wv.most_similar(positive=['inspector','professor'], negative=['lestrade']))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Our model recongnizes inspectors and knows Watson isn't one\")\n",
    "print(\"Given : gregson watson lestrade baynes\")\n",
    "print(\"Output : \", model.wv.doesnt_match(\"gregson watson lestrade baynes\".split()))\n",
    "print()\n",
    "\n",
    "print(\"Wow! it knows the difference between the good guys and the bad guys !\")\n",
    "print(\"Given : moritarty holmes augustus sebastian\")\n",
    "print(\"Output : \",model.wv.doesnt_match(\"moritarty holmes augustus sebastian\".split()))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
